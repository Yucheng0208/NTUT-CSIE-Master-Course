{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-arithmetic\n",
    "\n",
    "## Dataset\n",
    "- [Arithmetic dataset](https://drive.google.com/file/d/1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Matplotlib 的進階視覺庫\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split # 切分資料\n",
    "\n",
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2285313</td>\n",
       "      <td>14*(43+20)=</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317061</td>\n",
       "      <td>(6+1)*5=</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718770</td>\n",
       "      <td>13+32+29=</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170195</td>\n",
       "      <td>31*(3-11)=</td>\n",
       "      <td>-248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2581417</td>\n",
       "      <td>24*49+1=</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          src   tgt\n",
       "0     2285313  14*(43+20)=   882\n",
       "1      317061     (6+1)*5=    35\n",
       "2      718770    13+32+29=    74\n",
       "3      170195   31*(3-11)=  -248\n",
       "4     2581417     24*49+1=  1177"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_path, 'arithmetic_train.csv'))\n",
    "df_eval = pd.read_csv(os.path.join(data_path, 'arithmetic_eval.csv'))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the input data to string\n",
    "df_train['tgt'] = df_train['tgt'].apply(lambda x: str(x))\n",
    "#df_train['src'] = df_train['src'].add(df_train['tgt'])\n",
    "df_train['len'] = df_train['src'].apply(lambda x: len(x))\n",
    "\n",
    "df_eval['tgt'] = df_eval['tgt'].apply(lambda x: str(x))\n",
    "#df_eval['src'] = df_eval['src'].add(df_eval['tgt'])\n",
    "df_eval['len'] = df_eval['src'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dictionary\n",
    " - The model cannot perform calculations directly with plain text.\n",
    " - Convert all text (numbers/symbols) into numerical representations.\n",
    " - Special tokens\n",
    "    - '&lt;pad&gt;'\n",
    "        - Each sentence within a batch may have different lengths.\n",
    "        - The length is padded with '&lt;pad&gt;' to match the longest sentence in the batch.\n",
    "    - '&lt;eos&gt;'\n",
    "        - Specifies the end of the generated sequence.\n",
    "        - Without '&lt;eos&gt;', the model will not know when to stop generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 18\n"
     ]
    }
   ],
   "source": [
    "char_to_id = {}\n",
    "id_to_char = {}\n",
    "\n",
    "# 建立字典並為訓練數據集中的每個標記分配ID\n",
    "# 字典應包含 <eos> 和 <pad>\n",
    "# char_to_id 用於將字符轉換為ID，而 id_to_char 則相反\n",
    "special_tokens = ['<pad>', '<eos>']\n",
    "\n",
    "# 包含 src 和 tgt 中的所有字符\n",
    "all_chars = set(''.join(df_train['src']) + ''.join(df_train['tgt']) + ''.join(df_eval['src']) + ''.join(df_eval['tgt']))\n",
    "\n",
    "# 分離數字和符號（可選，根據需要）\n",
    "digits = sorted([char for char in all_chars if char.isdigit()])\n",
    "symbols = sorted([char for char in all_chars if not char.isdigit()])\n",
    "\n",
    "# 分配特殊標記的 ID\n",
    "char_to_id = {'<pad>': 0, '<eos>': 1}\n",
    "\n",
    "# 分配其他字符的 ID\n",
    "char_to_id.update({char: idx + 2 for idx, char in enumerate(digits + symbols)})\n",
    "id_to_char = {idx: char for char, idx in char_to_id.items()}\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "print('Vocab size: {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  Character\n",
      "0   <pad>          0\n",
      "1   <eos>          1\n",
      "2       0          2\n",
      "3       1          3\n",
      "4       2          4\n",
      "5       3          5\n",
      "6       4          6\n",
      "7       5          7\n",
      "8       6          8\n",
      "9       7          9\n",
      "10      8         10\n",
      "11      9         11\n",
      "12      (         12\n",
      "13      )         13\n",
      "14      *         14\n",
      "15      +         15\n",
      "16      -         16\n",
      "17      =         17\n"
     ]
    }
   ],
   "source": [
    "# 檢查數據表\n",
    "char_to_id_df = pd.DataFrame(list(char_to_id.items()), columns=['ID', 'Character'])\n",
    "print(char_to_id_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    " - The data is processed into the format required for the model's input and output.\n",
    " - Example: 1+2-3=0\n",
    "     - Model input: 1 + 2 - 3 = 0\n",
    "     - Model output: / / / / / 0 &lt;eos&gt;  (the '/' can be replaced with &lt;pad&gt;)\n",
    "     - The key for the model's output is that the model does not need to predict the next character of the previous part. What matters is that once the model sees '=', it should start generating the answer, which is '0'. After generating the answer, it should also generate&lt;eos&gt;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dictionary\n",
    " - The model cannot perform calculations directly with plain text.\n",
    " - Convert all text (numbers/symbols) into numerical representations.\n",
    " - Special tokens\n",
    "    - '&lt;pad&gt;'\n",
    "        - Each sentence within a batch may have different lengths.\n",
    "        - The length is padded with '&lt;pad&gt;' to match the longest sentence in the batch.\n",
    "    - '&lt;eos&gt;'\n",
    "        - Specifies the end of the generated sequence.\n",
    "        - Without '&lt;eos&gt;', the model will not know when to stop generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>len</th>\n",
       "      <th>char_id_list</th>\n",
       "      <th>label_id_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14*(43+20)=</td>\n",
       "      <td>882</td>\n",
       "      <td>11</td>\n",
       "      <td>[3, 6, 14, 12, 6, 5, 15, 4, 2, 13, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(6+1)*5=</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>[12, 8, 15, 3, 13, 14, 7, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13+32+29=</td>\n",
       "      <td>74</td>\n",
       "      <td>9</td>\n",
       "      <td>[3, 5, 15, 5, 4, 15, 4, 11, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 6, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31*(3-11)=</td>\n",
       "      <td>-248</td>\n",
       "      <td>10</td>\n",
       "      <td>[5, 3, 14, 12, 5, 16, 3, 3, 13, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 16, 4, 6, 10, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24*49+1=</td>\n",
       "      <td>1177</td>\n",
       "      <td>8</td>\n",
       "      <td>[4, 6, 14, 6, 11, 15, 3, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 9, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           src   tgt  len                               char_id_list  \\\n",
       "0  14*(43+20)=   882   11  [3, 6, 14, 12, 6, 5, 15, 4, 2, 13, 17, 1]   \n",
       "1     (6+1)*5=    35    8           [12, 8, 15, 3, 13, 14, 7, 17, 1]   \n",
       "2    13+32+29=    74    9         [3, 5, 15, 5, 4, 15, 4, 11, 17, 1]   \n",
       "3   31*(3-11)=  -248   10     [5, 3, 14, 12, 5, 16, 3, 3, 13, 17, 1]   \n",
       "4     24*49+1=  1177    8            [4, 6, 14, 6, 11, 15, 3, 17, 1]   \n",
       "\n",
       "                            label_id_list  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 4, 1]  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 1]  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 6, 1]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 16, 4, 6, 10, 1]  \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 9, 1]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[['src', 'tgt', 'len']]\n",
    "df_train['char_id_list'] = df_train['src'].apply(lambda x: [char_to_id[char] for char in x] + [char_to_id['<eos>']])\n",
    "df_train['label_id_list'] = df_train['tgt'].apply(lambda y: [char_to_id['<pad>']] * (len(df_train['src'].iloc[0]) - len(y)) + [char_to_id[char] for char in y] + [char_to_id['<eos>']])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>len</th>\n",
       "      <th>char_id_list</th>\n",
       "      <th>label_id_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48+43+34=</td>\n",
       "      <td>125</td>\n",
       "      <td>9</td>\n",
       "      <td>[6, 10, 15, 6, 5, 15, 5, 6, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 4, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-(48+13)=</td>\n",
       "      <td>-31</td>\n",
       "      <td>11</td>\n",
       "      <td>[5, 2, 16, 12, 6, 10, 15, 3, 5, 13, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 16, 5, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(21*31)+10=</td>\n",
       "      <td>661</td>\n",
       "      <td>11</td>\n",
       "      <td>[12, 4, 3, 14, 5, 3, 13, 15, 3, 2, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 8, 8, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-27-10=</td>\n",
       "      <td>-35</td>\n",
       "      <td>8</td>\n",
       "      <td>[4, 16, 4, 9, 16, 3, 2, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 16, 5, 7, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(15*20)+24=</td>\n",
       "      <td>324</td>\n",
       "      <td>11</td>\n",
       "      <td>[12, 3, 7, 14, 4, 2, 13, 15, 4, 6, 17, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 5, 4, 6, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           src  tgt  len                                char_id_list  \\\n",
       "0    48+43+34=  125    9          [6, 10, 15, 6, 5, 15, 5, 6, 17, 1]   \n",
       "1  30-(48+13)=  -31   11  [5, 2, 16, 12, 6, 10, 15, 3, 5, 13, 17, 1]   \n",
       "2  (21*31)+10=  661   11   [12, 4, 3, 14, 5, 3, 13, 15, 3, 2, 17, 1]   \n",
       "3     2-27-10=  -35    8              [4, 16, 4, 9, 16, 3, 2, 17, 1]   \n",
       "4  (15*20)+24=  324   11   [12, 3, 7, 14, 4, 2, 13, 15, 4, 6, 17, 1]   \n",
       "\n",
       "                     label_id_list  \n",
       "0   [0, 0, 0, 0, 0, 0, 3, 4, 7, 1]  \n",
       "1  [0, 0, 0, 0, 0, 0, 16, 5, 3, 1]  \n",
       "2   [0, 0, 0, 0, 0, 0, 8, 8, 3, 1]  \n",
       "3  [0, 0, 0, 0, 0, 0, 16, 5, 7, 1]  \n",
       "4   [0, 0, 0, 0, 0, 0, 5, 4, 6, 1]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = df_eval[['src', 'tgt', 'len']]\n",
    "df_eval['char_id_list'] = df_eval['src'].apply(lambda x: [char_to_id[char] for char in x] + [char_to_id['<eos>']])\n",
    "df_eval['label_id_list'] = df_eval['tgt'].apply(lambda y: [char_to_id['<pad>']] * (len(df_eval['src'].iloc[0]) - len(y)) + [char_to_id[char] for char in y] + [char_to_id['<eos>']])\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters\n",
    "\n",
    "|Hyperparameter|Meaning|Value|\n",
    "|-|-|-|\n",
    "|`batch_size`|Number of data samples in a single batch|64|\n",
    "|`epochs`|Total number of epochs to train|10|\n",
    "|`embed_dim`|Dimension of the word embeddings|256|\n",
    "|`hidden_dim`|Dimension of the hidden state in each timestep of the LSTM|256|\n",
    "|`lr`|Learning Rate|0.001|\n",
    "|`grad_clip`|To prevent gradient explosion in RNNs, restrict the gradient range|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "embed_dim = 256\n",
    "hidden_dim = 256\n",
    "lr = 0.0004\n",
    "grad_clip = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Batching\n",
    "- Use `torch.utils.data.Dataset` to create a data generation tool called  `dataset`.\n",
    "- The, use `torch.utils.data.DataLoader` to randomly sample from the `dataset` and group the samples into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return the amount of data\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Extract the input data x and the ground truth y from the data\n",
    "        x = self.sequences[index][0]\n",
    "        y = self.sequences[index][1]\n",
    "        return x, y\n",
    "\n",
    "# collate function, used to build dataloader\n",
    "def collate_fn(batch):\n",
    "    batch_x = [torch.tensor(data[0]) for data in batch]\n",
    "    batch_y = [torch.tensor(data[1]) for data in batch]\n",
    "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
    "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
    "    \n",
    "    # Pad the input sequence\n",
    "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(batch_x,\n",
    "                                                  batch_first=True,\n",
    "                                                  padding_value=char_to_id['<pad>'])\n",
    "    \n",
    "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(batch_y,\n",
    "                                                  batch_first=True,\n",
    "                                                  padding_value=char_to_id['<pad>'])\n",
    "    \n",
    "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset(df_train[['char_id_list', 'label_id_list']].values.tolist())\n",
    "ds_eval = Dataset(df_eval[['char_id_list', 'label_id_list']].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader of train set and eval set, collate_fn is the collate function\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "dl_eval = torch.utils.data.DataLoader(ds_eval, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design\n",
    "\n",
    "## Execution Flow\n",
    "1. Convert all characters in the sentence into embeddings.\n",
    "2. Pass the embeddings through an LSTM sequentially.\n",
    "3. The output of the LSTM is passed into another LSTM, and additional layers can be added.\n",
    "4. The output from all time steps of the final LSTM is passed through a Fully Connected layer.\n",
    "5. The character corresponding to the maximum value across all output dimensions is selected as the next character.\n",
    "\n",
    "## Loss Function\n",
    "Since this is a classification task, Cross Entropy is used as the loss function.\n",
    "\n",
    "## Gradient Update\n",
    "Adam algorithm is used for gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=embed_dim,\n",
    "                                            padding_idx=char_to_id['<pad>'])\n",
    "        \n",
    "        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.3)  # Dropout layer\n",
    "\n",
    "        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,\n",
    "                                        hidden_size=hidden_dim,\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
    "            torch.nn.LeakyReLU(),  # LeakyReLU for more stability\n",
    "            torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "    \n",
    "    # The forward pass of the model\n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        # 將字符轉換為嵌入層，這部分保持在 GPU 上\n",
    "        batch_x = self.embedding(batch_x)\n",
    "        \n",
    "        # 只將 batch_x_lens 移到 CPU 上，其餘保持在 GPU 上\n",
    "        packed_input = torch.nn.utils.rnn.pack_padded_sequence(batch_x,\n",
    "                                                            batch_x_lens,  # 確保 lengths 在 CPU 上\n",
    "                                                            batch_first=True,\n",
    "                                                            enforce_sorted=False)\n",
    "        \n",
    "        # 通過第一層 LSTM，這部分保持在 GPU 上\n",
    "        packed_output, _ = self.rnn_layer1(packed_input)\n",
    "        \n",
    "        # 通過第二層 LSTM，這部分保持在 GPU 上\n",
    "        packed_output, _ = self.rnn_layer2(packed_output)\n",
    "        \n",
    "        # 將序列轉換回填充的形式\n",
    "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output,\n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        # 通過線性層\n",
    "        batch_x = self.linear(batch_x)\n",
    "        \n",
    "        return batch_x\n",
    "    \n",
    "    # 修正的生成器方法\n",
    "    def generator(self, start_char, max_len=50):\n",
    "        # 初始序列\n",
    "        char_list = [char_to_id[c] for c in start_char]\n",
    "        input_tensor = torch.tensor([char_list], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        \n",
    "        # 初始化隱藏狀態，初始化為零\n",
    "        hidden_state1 = (torch.zeros(1, 1, self.rnn_layer1.hidden_size).to(next(self.parameters()).device),\n",
    "                         torch.zeros(1, 1, self.rnn_layer1.hidden_size).to(next(self.parameters()).device))\n",
    "        hidden_state2 = (torch.zeros(1, 1, self.rnn_layer2.hidden_size).to(next(self.parameters()).device),\n",
    "                         torch.zeros(1, 1, self.rnn_layer2.hidden_size).to(next(self.parameters()).device))\n",
    "        \n",
    "        # 逐步生成字符\n",
    "        while len(char_list) < max_len: \n",
    "            # 將 char_list 轉換為 tensor 並嵌入\n",
    "            input_tensor = torch.tensor([[char_list[-1]]], dtype=torch.long).to(next(self.parameters()).device)\n",
    "            embedded = self.embedding(input_tensor)\n",
    "\n",
    "            # 前向傳播到 LSTM 層\n",
    "            output, hidden_state1 = self.rnn_layer1(embedded, hidden_state1)\n",
    "            output, hidden_state2 = self.rnn_layer2(output, hidden_state2)\n",
    "            \n",
    "            # 通過線性層獲取預測\n",
    "            output = self.linear(output)\n",
    "            y = output[:, -1, :]\n",
    "            \n",
    "            # 獲取概率最大的字符\n",
    "            next_char = torch.argmax(y, dim=1).item()\n",
    "            \n",
    "            # 如果是 <eos>，則停止生成\n",
    "            if next_char == char_to_id['<eos>']:\n",
    "                break\n",
    "            \n",
    "            # 否則，添加到 char_list 中\n",
    "            char_list.append(next_char)\n",
    "        \n",
    "        # 將 ID 轉換回字符並返回\n",
    "        return ''.join([id_to_char[ch_id] for ch_id in char_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is ready\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == 'cuda': print(\"GPU is ready\")\n",
    "else: print(\"CPU is ready\")\n",
    "\n",
    "model = CharRNN(vocab_size,\n",
    "                embed_dim,\n",
    "                hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 定義損失函數，使用交叉熵損失，並忽略 <pad> 標記\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])\n",
    "\n",
    "# 定義優化器，使用 Adam 或 AdamW\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# 或者使用 AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "1. The outer `for` loop controls the `epoch`\n",
    "    1. The inner `for` loop uses `data_loader` to retrieve batches.\n",
    "        1. Pass the batch to the `model` for training.\n",
    "        2. Compare the predicted results `batch_pred_y` with the true labels `batch_y` using Cross Entropy to calculate the loss `loss`\n",
    "        3. Use `loss.backward` to automatically compute the gradients.\n",
    "        4. Use `torch.nn.utils.clip_grad_value_` to limit the gradient values between `-grad_clip` &lt; and &lt; `grad_clip`.\n",
    "        5. Use `optimizer.step()` to update the model (backpropagation).\n",
    "2.  After every `1000` batches, output the current loss to monitor whether it is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|██████████| 37020/37020 [03:31<00:00, 175.44it/s, loss=1.22] \n",
      "Validation epoch 1: 100%|██████████| 4114/4114 [03:33<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 1: 0.9553010446343779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|██████████| 37020/37020 [03:25<00:00, 180.21it/s, loss=1.08] \n",
      "Validation epoch 2: 100%|██████████| 4114/4114 [03:24<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 2: 0.9551377018043685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3: 100%|██████████| 37020/37020 [03:35<00:00, 171.65it/s, loss=1.03] \n",
      "Validation epoch 3: 100%|██████████| 4114/4114 [04:10<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 3: 0.9551073124406457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4: 100%|██████████| 37020/37020 [03:29<00:00, 176.90it/s, loss=0.883]\n",
      "Validation epoch 4: 100%|██████████| 4114/4114 [03:59<00:00, 17.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 4: 0.9551528964862298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 5: 100%|██████████| 37020/37020 [03:27<00:00, 178.37it/s, loss=0.888]\n",
      "Validation epoch 5: 100%|██████████| 4114/4114 [03:36<00:00, 19.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 5: 0.9551566951566951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 6: 100%|██████████| 37020/37020 [03:20<00:00, 184.89it/s, loss=0.838]\n",
      "Validation epoch 6: 100%|██████████| 4114/4114 [03:44<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 6: 0.9551452991452991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 7: 100%|██████████| 37020/37020 [03:27<00:00, 178.62it/s, loss=0.754]\n",
      "Validation epoch 7: 100%|██████████| 4114/4114 [03:47<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 7: 0.9550237416904084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 8: 100%|██████████| 37020/37020 [03:20<00:00, 184.24it/s, loss=0.795]\n",
      "Validation epoch 8: 100%|██████████| 4114/4114 [03:41<00:00, 18.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 8: 0.9550123456790124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 9: 100%|██████████| 37020/37020 [03:19<00:00, 185.51it/s, loss=0.773]\n",
      "Validation epoch 9: 100%|██████████| 4114/4114 [03:37<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 9: 0.9550009496676163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 10: 100%|██████████| 37020/37020 [03:20<00:00, 184.45it/s, loss=0.852]\n",
      "Validation epoch 10: 100%|██████████| 4114/4114 [03:36<00:00, 18.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) at epoch 10: 0.9550237416904084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "model = model.to(device)\n",
    "best_em = 0.0\n",
    "i = 0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    bar = tqdm(dl_train, desc=f\"Train epoch {epoch}\")\n",
    "    for batch_x, batch_y, batch_x_lens, batch_y_lens in bar:\n",
    "        optimizer.zero_grad()\n",
    "        batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
    "        \n",
    "        max_length = max(batch_y.shape[1], batch_pred_y.shape[1])\n",
    "\n",
    "        if batch_y.shape[1] < max_length:\n",
    "            pad_size = max_length - batch_y.shape[1]\n",
    "            pad_tensor = torch.full((batch_y.shape[0], pad_size), fill_value=char_to_id['<pad>']).to(device)\n",
    "            batch_y = torch.cat([batch_y.to(device), pad_tensor], dim=1)\n",
    "\n",
    "        if batch_pred_y.shape[1] < max_length:\n",
    "            pad_size = max_length - batch_pred_y.shape[1]\n",
    "            pad_tensor = torch.zeros((batch_pred_y.shape[0], pad_size, vocab_size)).to(device)\n",
    "            batch_pred_y = torch.cat([batch_pred_y, pad_tensor], dim=1)\n",
    "\n",
    "        pred_y_reshaped = batch_pred_y.reshape(-1, vocab_size)\n",
    "        y_reshaped = batch_y.reshape(-1).to(device)\n",
    "\n",
    "        loss = criterion(pred_y_reshaped, y_reshaped)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        i += 1\n",
    "        if i % 50 == 0:\n",
    "            bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation phase\n",
    "    bar = tqdm(dl_eval, desc=f\"Validation epoch {epoch}\")\n",
    "    matched = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch_x, batch_y, batch_x_lens, batch_y_lens in bar:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch_x, batch_x_lens)\n",
    "\n",
    "        predicted_ids = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "        for pred, true in zip(predicted_ids, batch_y):\n",
    "            min_length = min(len(pred), len(true))\n",
    "            pred = pred[:min_length]\n",
    "            true = true[:min_length]\n",
    "\n",
    "            for j in range(min_length):\n",
    "                if true[j].cpu().item() == char_to_id['<pad>']:\n",
    "                    pred[j] = char_to_id['<pad>']\n",
    "\n",
    "            true_length = (true != char_to_id['<pad>']).sum().item()\n",
    "            if true_length == 0:\n",
    "                continue\n",
    "\n",
    "            pred_length = min(len(pred), true_length)\n",
    "            if np.array_equal(pred[:pred_length].cpu().numpy(), true[:pred_length].cpu().numpy()):\n",
    "                matched += 1\n",
    "\n",
    "        total += len(batch_y)\n",
    "\n",
    "    em = matched / total if total > 0 else 0\n",
    "    print(f\"Exact Match (EM) at epoch {epoch}: {em}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if em > best_em:\n",
    "        best_em = em\n",
    "        best_model = deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "Use `model.generator` and provide an initial character to automatically generate a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cpu\")\n",
    "print(\"\".join(model.generator('1+1=')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
