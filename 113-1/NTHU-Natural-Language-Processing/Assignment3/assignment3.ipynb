{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as T\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs('./saved_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T.BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example: \n",
      "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.200000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 3, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.699999809265137, 'entailment_judgment': 1}\n"
     ]
    }
   ],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\", split=split, cache_dir=\"./cache/\"\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # 把中文標點替換掉\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "lr = 5e-5\n",
    "epochs = 3\n",
    "train_batch_size = 4\n",
    "validation_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    premises, hypotheses, relatedness_scores, entailment_judgments = zip(\n",
    "        *[(item[\"premise\"], item[\"hypothesis\"], item[\"relatedness_score\"], item[\"entailment_judgment\"]) for item in batch]\n",
    "    )\n",
    "    \n",
    "    # Tokenize the premises and hypotheses\n",
    "    encodings = tokenizer(premises, hypotheses, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Convert relatedness scores and entailment judgments to tensors\n",
    "    relatedness_scores = torch.tensor(relatedness_scores, dtype=torch.float)\n",
    "    entailment_judgments = torch.tensor(entailment_judgments, dtype=torch.long)\n",
    "    \n",
    "    return encodings, relatedness_scores, entailment_judgments\n",
    "\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "dl_train = DataLoader(SemevalDataset(split=\"train\"), batch_size=train_batch_size, collate_fn=collate_fn)\n",
    "dl_validation = DataLoader(SemevalDataset(split=\"validation\"), batch_size=validation_batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(MultiLabelModel, self).__init__()\n",
    "        # 使用 Hugging Face 的 BERT 模型\n",
    "        self.bert = T.BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # 線性層用於輸出相關性分數 (regression)\n",
    "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        \n",
    "        # 線性層用於輸出推理判斷 (classification)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 3)  # 假設推理判斷有三個類別\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # 使用 BERT 模型進行前向傳播\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        # 獲取 [CLS] 標記的輸出（第0個位置）\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 預測相關性分數\n",
    "        relatedness_score = self.regressor(cls_output).squeeze(-1)  # 轉換形狀為 (batch_size,)\n",
    "        \n",
    "        # 預測推理判斷\n",
    "        entailment_judgment = self.classifier(cls_output)  # 輸出形狀為 (batch_size, num_classes)\n",
    "        \n",
    "        return relatedness_score, entailment_judgment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(MultiLabelModel, self).__init__()\n",
    "        # 使用 Hugging Face 的 BERT 模型\n",
    "        self.bert = T.BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # 線性層用於輸出相關性分數 (regression)\n",
    "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        \n",
    "        # 線性層用於輸出推理判斷 (classification)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 3)  # 假設推理判斷有三個類別\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # 使用 BERT 模型進行前向傳播\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        # 獲取 [CLS] 標記的輸出（第0個位置）\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 預測相關性分數\n",
    "        relatedness_score = self.regressor(cls_output).squeeze(-1)  # 轉換形狀為 (batch_size,)\n",
    "        \n",
    "        # 預測推理判斷\n",
    "        entailment_judgment = self.classifier(cls_output)  # 輸出形狀為 (batch_size, num_classes)\n",
    "        \n",
    "        return relatedness_score, entailment_judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "# TODO3-1: Define your Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "# 損失函數1：回歸任務使用 MSE 損失\n",
    "regression_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# 損失函數2：分類任務使用 Cross-Entropy 損失\n",
    "classification_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# scoring functions\n",
    "spc = SpearmanCorrCoef()\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/3]: 100%|██████████| 188/188 [00:09<00:00, 20.21it/s, loss=0.0308]\n",
      "Validation epoch [1/3]: 100%|██████████| 21/21 [00:00<00:00, 58.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.7704\n",
      "Accuracy: 0.810000\n",
      "F1 Score: 0.813793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/3]: 100%|██████████| 188/188 [00:08<00:00, 20.91it/s, loss=0.0516]\n",
      "Validation epoch [2/3]: 100%|██████████| 21/21 [00:00<00:00, 59.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.7736\n",
      "Accuracy: 0.810000\n",
      "F1 Score: 0.816651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/3]: 100%|██████████| 188/188 [00:09<00:00, 20.83it/s, loss=0.0535]\n",
      "Validation epoch [3/3]: 100%|██████████| 21/21 [00:00<00:00, 58.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.8251\n",
      "Accuracy: 0.866000\n",
      "F1 Score: 0.863009\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # 清除上一步的梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 提取輸入和標籤\n",
    "        encodings, relatedness_scores, entailment_judgments = batch\n",
    "        \n",
    "        # 將數據移動到可用設備\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        relatedness_scores = relatedness_scores.to(device)\n",
    "        entailment_judgments = entailment_judgments.to(device)\n",
    "        \n",
    "        # 前向傳播\n",
    "        pred_relatedness, pred_entailment = model(**encodings)\n",
    "        \n",
    "        # 計算損失\n",
    "        loss_relatedness = regression_loss_fn(pred_relatedness, relatedness_scores)\n",
    "        loss_entailment = classification_loss_fn(pred_entailment, entailment_judgments)\n",
    "        total_loss = loss_relatedness + loss_entailment  # 總損失為兩者相加\n",
    "        \n",
    "        # 反向傳播和優化\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新進度條的描述\n",
    "        pbar.set_postfix(loss=total_loss.item())\n",
    "    \n",
    "    # 驗證階段\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    \n",
    "    # 初始化計算指標的變量\n",
    "    all_pred_relatedness = []\n",
    "    all_true_relatedness = []\n",
    "    all_pred_entailment = []\n",
    "    all_true_entailment = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            encodings, relatedness_scores, entailment_judgments = batch\n",
    "            \n",
    "            # 將數據移動到可用設備\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            relatedness_scores = relatedness_scores.to(device)\n",
    "            entailment_judgments = entailment_judgments.to(device)\n",
    "            \n",
    "            # 前向傳播\n",
    "            pred_relatedness, pred_entailment = model(**encodings)\n",
    "            \n",
    "            # 收集預測和真實值\n",
    "            all_pred_relatedness.extend(pred_relatedness.cpu().numpy())\n",
    "            all_true_relatedness.extend(relatedness_scores.cpu().numpy())\n",
    "            all_pred_entailment.extend(pred_entailment.argmax(dim=1).cpu().numpy())\n",
    "            all_true_entailment.extend(entailment_judgments.cpu().numpy())\n",
    "    \n",
    "    # 計算評估指標\n",
    "    spc_score = spc(torch.tensor(all_pred_relatedness), torch.tensor(all_true_relatedness))\n",
    "    acc_score = acc(torch.tensor(all_pred_entailment), torch.tensor(all_true_entailment))\n",
    "    f1_score = f1(torch.tensor(all_pred_entailment), torch.tensor(all_true_entailment))\n",
    "    \n",
    "    print(f\"Spearman Corr: {spc_score:.4f}\")\n",
    "    print(f\"Accuracy: {acc_score:4f}\")\n",
    "    print(f\"F1 Score: {f1_score:4f}\")\n",
    "    \n",
    "    # 儲存模型\n",
    "    torch.save(model, f'./saved_models/ep{ep}.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\AppData\\Local\\Temp\\ipykernel_37848\\114888944.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('./saved_models/ep2.ckpt')\n",
      "c:\\Users\\Ryan\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Testing: 100%|██████████| 1232/1232 [00:10<00:00, 122.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr on Test Set: 0.8163201808929443\n",
      "Bonus: 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 確認你能夠正確測試模型以及判斷給予的加分\n",
    "\n",
    "# 載入存檔的模型\n",
    "model = torch.load('./saved_models/ep2.ckpt')\n",
    "model.eval()\n",
    "\n",
    "# 修正 SemevalDataset 義類以支援 test 分割\n",
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]  # 增加 \"test\" 支援\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\", split=split, cache_dir=\"./cache/\"\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # 把中文標點替換掉\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# 準備測試資料集 DataLoader\n",
    "dl_test = DataLoader(SemevalDataset(split=\"test\"), batch_size=validation_batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# 初始化計算指標的變量\n",
    "all_pred_relatedness = []\n",
    "all_true_relatedness = []\n",
    "\n",
    "# 將 SpearmanCorrCoef 引入\n",
    "from torchmetrics import SpearmanCorrCoef\n",
    "spc = SpearmanCorrCoef()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_test, desc=\"Testing\"):\n",
    "        encodings, relatedness_scores, entailment_judgments = batch\n",
    "        \n",
    "        # 移動數據到設備\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "        relatedness_scores = relatedness_scores.to(device)\n",
    "        \n",
    "        # 前向傳播\n",
    "        pred_relatedness, _ = model(**encodings)\n",
    "        \n",
    "        # 收集預測和真實值\n",
    "        all_pred_relatedness.extend(pred_relatedness.cpu().numpy())\n",
    "        all_true_relatedness.extend(relatedness_scores.cpu().numpy())\n",
    "\n",
    "# 計算 Spearman correlation coefficient\n",
    "spc_score = spc(torch.tensor(all_pred_relatedness), torch.tensor(all_true_relatedness))\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"Spearman Corr on Test Set: {spc_score}\")\n",
    "\n",
    "# 確認 Bonus 分數\n",
    "if spc_score > 0.8:\n",
    "    print(\"Bonus: 10%\")\n",
    "elif spc_score > 0.77:\n",
    "    print(\"Bonus: 7%\")\n",
    "elif spc_score > 0.74:\n",
    "    print(\"Bonus: 3%\")\n",
    "else:\n",
    "    print(\"No Bonus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
