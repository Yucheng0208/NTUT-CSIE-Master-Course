{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Set memory growth for GPU to avoid full memory allocation\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "DATA_PATH = 'D:/NTHU_NLP_2024_Term_Project_35/'\n",
    "train_essays = pd.read_csv(f'{DATA_PATH}/train_essays.csv')\n",
    "prompts = pd.read_csv(f'{DATA_PATH}/train_prompts.csv')\n",
    "train_v2 = pd.read_csv(f'{DATA_PATH}/train_v2_drcat_02.csv')\n",
    "train_lim = pd.read_csv(f'{DATA_PATH}/ai_generated_train_essays.csv')\n",
    "train_lim2 = pd.read_csv(f'{DATA_PATH}/ai_generated_train_essays_gpt-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "combined_from_comp = pd.merge(train_essays, prompts, on='prompt_id', how='left')\n",
    "train_lim = pd.concat([train_lim, train_lim2], ignore_index=True)\n",
    "train_merged = pd.concat([combined_from_comp, train_lim], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def remove_tag(text):\n",
    "    tag = re.compile(r'@\\S+')\n",
    "    return tag.sub(r'', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url, '', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<[^>]+>|\\([^)]+\\)')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    punctuations = list(string.punctuation)\n",
    "    table = str.maketrans('', '', ''.join(punctuations))\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning\n",
    "train_merged['cleaned'] = train_merged['text'].apply(lambda x: remove_tag(x))\n",
    "train_merged['cleaned'] = train_merged['cleaned'].apply(lambda x: remove_URL(x))\n",
    "train_merged['cleaned'] = train_merged['cleaned'].apply(lambda x: remove_html(x))\n",
    "train_merged['cleaned'] = train_merged['cleaned'].apply(lambda x: remove_punct(x))\n",
    "train_merged['cleaned'] = train_merged['cleaned'].apply(lambda x: x.lower())\n",
    "train_merged['cleaned'] = train_merged['cleaned'].apply(lambda x: nltk.word_tokenize(x))\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "train_merged['cleaned'] = train_merged['cleaned'].apply(lambda x: ' '.join([word for word in x if word not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing dataset\n",
    "majority_class = train_merged[train_merged['generated'] == 0]\n",
    "minority_class = train_merged[train_merged['generated'] == 1]\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "train_merged = pd.concat([majority_downsampled, minority_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_val_df, test_df = train_test_split(train_merged, test_size=0.1, random_state=42, stratify=train_merged['generated'])\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42, stratify=train_val_df['generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and encoding\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_df['cleaned'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['cleaned'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['cleaned'].tolist(), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_df['generated'].tolist()))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_df['generated'].tolist()))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_df['generated'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch datasets\n",
    "train_dataset = train_dataset.shuffle(len(train_dataset)).batch(8)\n",
    "val_dataset = val_dataset.batch(16)\n",
    "test_dataset = test_dataset.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create strategy for multi-GPU training if multiple GPUs are available\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the model within strategy scope\n",
    "with strategy.scope():\n",
    "    base_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "    class DistilBertWithDropout(tf.keras.Model):\n",
    "        def __init__(self, base_model, num_labels, dropout_rate=0.3):\n",
    "            super(DistilBertWithDropout, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "            self.classifier = tf.keras.layers.Dense(num_labels, activation='softmax')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            outputs = self.base_model(inputs)\n",
    "            pooled_output = outputs[0][:, 0, :]\n",
    "            dropout_output = self.dropout(pooled_output, training=training)\n",
    "            logits = self.classifier(dropout_output)\n",
    "            return logits\n",
    "\n",
    "    model = DistilBertWithDropout(base_model, num_labels=2)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"best_model\", save_best_only=True, save_format='tf')\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=4, callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred_labels = tf.argmax(y_pred, axis=1)\n",
    "y_true = test_df['generated'].values\n",
    "cf_matrix = confusion_matrix(y_true, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "categories = ['Negative', 'Positive']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, display_labels=categories)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('AI-detector', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_termproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
